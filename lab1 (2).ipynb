{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0CRSEN2517-2023-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Winning Techniques for Your Next Data Science Contest**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of data science is rapidly growing, and the need for individuals in the traditional industries, such as Agriculture, Transportation, Construction, Retail, Hospitality and Tourism, to acquire these skills is also increasing. If you are an expert in these industries and would like to have a competitive edge in your career and the ability to drive innovation in your industry, then you found the right place!\n",
    "\n",
    "\n",
    "This guided project provides a unique opportunity for you to learn about data science and machine learning in a practical way. We will guide you through the full machine learning project cycle, which typically includes the following steps:\n",
    "\n",
    "- **Problem formulation**: defining the problem and understanding the business context.\n",
    "\n",
    "- **Data collection**: gathering and acquiring data from various sources.\n",
    "\n",
    "- **Data preprocessing**: cleaning, transforming, and preparing data for analysis.\n",
    "\n",
    "- **Exploratory data analysis**: understanding the structure, contents, and patterns in the data through visualizations and summary statistics.\n",
    "\n",
    "- **Feature engineering**: creating new features from the existing data.\n",
    "\n",
    "- **Model selection**: choosing an appropriate model or algorithm for the problem.\n",
    "\n",
    "- **Model training**: fitting the model to the data and tuning its hyperparameters.\n",
    "\n",
    "- **Model evaluation**: assessing the performance of the model using appropriate metrics and techniques.\n",
    "\n",
    "\n",
    "This comprehensive guide is hard to find on the internet and will equip you with a solid foundation in the data science process.\n",
    "\n",
    "After completing the project, you will be able to use real-world data to make predictions and apply machine learning algorithms to analyze and draw insights from data.\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0CRSEN/images/Screen%20Shot%202023-04-03%20at%2010.51.22%20PM.png\" width=\"60%\"></center>\n",
    "\n",
    "<p style=\"color:gray; text-align:center;\">Credit: THINKSTOCK</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Background-(optional)\">Background (optional)</a></li>\n",
    "    <li><a href=\"#Loading the dataset\">Loading the dataset</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Preprocessing the dataset\">Preprocessing the dataset</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Make Loc Groups\">Make Loc Groups</a></li>\n",
    "            <li><a href=\"#Fill Missing Values\">Fill Missing Values</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Feature Selection and Lasso Regressions (to Obtain Residuals)\">Feature Selection and Lasso Regressions (to Obtain Residuals)</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Train, Validation, Test Split\">Train, Validation, Test Split</a></li>\n",
    "            <li><a href=\"#Feature Selection For Linear Regression\">Feature Selection For Linear Regression</a></li>\n",
    "            <li><a href=\"#Selecting Alpha and Fitting Linear Regressions (to Obtain Residuals)\">Selecting Alpha and Fitting Linear Regressions (to Obtain Residuals)</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Using Boosting Algorithms to Predict Residuals\">Using Boosting Algorithms to Predict Residuals</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Feature Selection for Boosting\">Feature Selection for Boosting</a></li>\n",
    "            <li><a href=\"#Boosting Models for Predicting Fitted Residuals\">Boosting Models for Predicting Fitted Residuals</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Learn the full machine learning project cycle, from data exploration to model evaluation\n",
    "- Gain hands-on experience with data cleaning and preparation techniques\n",
    "- Understand the importance of feature engineering and how to apply it to real-world datasets\n",
    "- Learn how to select and train machine learning models for different problems\n",
    "- Learn how to make predictions and draw insights from data using machine learning algorithms\n",
    "- Understand the working mechanisms of three famous boosting algorithms: CatBoost, LightGBM, and XGBoost.\n",
    "- Discover how to use ensemble learning techniques to combine model predictions and achieve even better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... \n",
      "\n",
      "    Installed package of scikit-learn can be accelerated using scikit-learn-intelex.\n",
      "    More details are available here: https://intel.github.io/scikit-learn-intelex\n",
      "\n",
      "    For example:\n",
      "\n",
      "        $ conda install scikit-learn-intelex\n",
      "        $ python -m sklearnex my_application.py\n",
      "\n",
      "    \n",
      "\n",
      "done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!mamba install -qy catboost\n",
    "!mamba install -qy lightgbm\n",
    "!mamba install -qy xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from numpy import median\n",
    "import random\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse(actual, predicted):\n",
    "    return mean_squared_error(actual, predicted, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Participating in data science competitions is a great way for beginners to gain practical experience and develop valuable data science skills. Competitions provide real-world datasets and problems that allow you to apply your knowledge and practice the entire data science process.\n",
    "\n",
    "The dataset we will use in this project is provided by Women in Data Science (WiDS Datathon) 2023 competition on Kaggle. This compeition focuses on forecasting weather in different regions within the US over a two-week period. Each row in the data corresponds to a single location and a single start date for the two-week period. **Our task is to predict the average temperature over the next 14 days, for each location and start date**.\n",
    "\n",
    "The competition is now finished and it had over 700 participating teams around the world. We collected the best winning techniques of the top winners of the compeition and built this project. We offer you this great opportunity to learn from the best approaches and elevate your data science skills!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0CRSEN/images/bg.png\" width=\"85%\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, in this project we will first preprocess the dataset by filling the missing values and encode valuable features. Then we will use Lasso regression to predict the residuals of our original target, which allows us to capture any simple, linear relationships existing between the predictors and the target. After we obtain the fitted residuals, we will bring on the power of Boosting algorithms, which we will introduce later in this project, to predict the fitted residuals. Boosting algorithms will help us capture complex, non-linear pattern in our data, and we hope to further reduce the gap between our predictions and the real target using Boosting algorithms.\n",
    "\n",
    "There are a lot of great technical details covered in this comprehensive project, so let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pre-prepared dataset consisting of features such as weather and climate information for a number of US locations, for a number of start dates for the two-week observation, as well as the forecasted temperature and precipitation from a number of weather forecast models. For more information regarding the data schema, please refer to the [competition](https://www.kaggle.com/competitions/widsdatathon2023/data?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0CRSEN2517-2023-01-01).\n",
    "\n",
    "Let's import the dataset. It may take 2-3 minutes to import as the original dataset is quite large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>startdate</th>\n",
       "      <th>contest-pevpr-sfc-gauss-14d__pevpr</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm30</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm40</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm30</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm40</th>\n",
       "      <th>nmme0-tmp2m-34w__cfsv20</th>\n",
       "      <th>nmme0-tmp2m-34w__gfdlflora0</th>\n",
       "      <th>...</th>\n",
       "      <th>wind-vwnd-925-2010-11</th>\n",
       "      <th>wind-vwnd-925-2010-12</th>\n",
       "      <th>wind-vwnd-925-2010-13</th>\n",
       "      <th>wind-vwnd-925-2010-14</th>\n",
       "      <th>wind-vwnd-925-2010-15</th>\n",
       "      <th>wind-vwnd-925-2010-16</th>\n",
       "      <th>wind-vwnd-925-2010-17</th>\n",
       "      <th>wind-vwnd-925-2010-18</th>\n",
       "      <th>wind-vwnd-925-2010-19</th>\n",
       "      <th>wind-vwnd-925-2010-20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>237.00</td>\n",
       "      <td>29.02</td>\n",
       "      <td>31.64</td>\n",
       "      <td>29.57</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.71</td>\n",
       "      <td>31.52</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.68</td>\n",
       "      <td>-37.21</td>\n",
       "      <td>8.32</td>\n",
       "      <td>9.56</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>48.13</td>\n",
       "      <td>28.09</td>\n",
       "      <td>-13.50</td>\n",
       "      <td>11.90</td>\n",
       "      <td>4.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2014-09-02</td>\n",
       "      <td>228.90</td>\n",
       "      <td>29.02</td>\n",
       "      <td>31.64</td>\n",
       "      <td>29.57</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.71</td>\n",
       "      <td>31.52</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.13</td>\n",
       "      <td>-36.57</td>\n",
       "      <td>8.77</td>\n",
       "      <td>21.17</td>\n",
       "      <td>4.44</td>\n",
       "      <td>48.60</td>\n",
       "      <td>27.41</td>\n",
       "      <td>-23.77</td>\n",
       "      <td>15.44</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2014-09-03</td>\n",
       "      <td>220.69</td>\n",
       "      <td>29.02</td>\n",
       "      <td>31.64</td>\n",
       "      <td>29.57</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.71</td>\n",
       "      <td>31.52</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.72</td>\n",
       "      <td>-34.16</td>\n",
       "      <td>6.99</td>\n",
       "      <td>32.16</td>\n",
       "      <td>5.01</td>\n",
       "      <td>48.53</td>\n",
       "      <td>19.21</td>\n",
       "      <td>-33.16</td>\n",
       "      <td>15.11</td>\n",
       "      <td>4.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2014-09-04</td>\n",
       "      <td>225.28</td>\n",
       "      <td>29.02</td>\n",
       "      <td>31.64</td>\n",
       "      <td>29.57</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.71</td>\n",
       "      <td>31.52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-31.04</td>\n",
       "      <td>6.17</td>\n",
       "      <td>39.66</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>50.59</td>\n",
       "      <td>8.29</td>\n",
       "      <td>-37.22</td>\n",
       "      <td>18.24</td>\n",
       "      <td>9.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2014-09-05</td>\n",
       "      <td>237.24</td>\n",
       "      <td>29.02</td>\n",
       "      <td>31.64</td>\n",
       "      <td>29.57</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.71</td>\n",
       "      <td>31.52</td>\n",
       "      <td>...</td>\n",
       "      <td>9.83</td>\n",
       "      <td>-31.80</td>\n",
       "      <td>7.47</td>\n",
       "      <td>38.62</td>\n",
       "      <td>-5.21</td>\n",
       "      <td>54.73</td>\n",
       "      <td>-2.58</td>\n",
       "      <td>-42.30</td>\n",
       "      <td>21.91</td>\n",
       "      <td>10.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 245 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lat       lon  startdate  contest-pevpr-sfc-gauss-14d__pevpr  \\\n",
       "index                                                                 \n",
       "0      0.0  0.833333 2014-09-01                              237.00   \n",
       "1      0.0  0.833333 2014-09-02                              228.90   \n",
       "2      0.0  0.833333 2014-09-03                              220.69   \n",
       "3      0.0  0.833333 2014-09-04                              225.28   \n",
       "4      0.0  0.833333 2014-09-05                              237.24   \n",
       "\n",
       "       nmme0-tmp2m-34w__cancm30  nmme0-tmp2m-34w__cancm40  \\\n",
       "index                                                       \n",
       "0                         29.02                     31.64   \n",
       "1                         29.02                     31.64   \n",
       "2                         29.02                     31.64   \n",
       "3                         29.02                     31.64   \n",
       "4                         29.02                     31.64   \n",
       "\n",
       "       nmme0-tmp2m-34w__ccsm30  nmme0-tmp2m-34w__ccsm40  \\\n",
       "index                                                     \n",
       "0                        29.57                    30.73   \n",
       "1                        29.57                    30.73   \n",
       "2                        29.57                    30.73   \n",
       "3                        29.57                    30.73   \n",
       "4                        29.57                    30.73   \n",
       "\n",
       "       nmme0-tmp2m-34w__cfsv20  nmme0-tmp2m-34w__gfdlflora0  ...  \\\n",
       "index                                                        ...   \n",
       "0                        29.71                        31.52  ...   \n",
       "1                        29.71                        31.52  ...   \n",
       "2                        29.71                        31.52  ...   \n",
       "3                        29.71                        31.52  ...   \n",
       "4                        29.71                        31.52  ...   \n",
       "\n",
       "       wind-vwnd-925-2010-11  wind-vwnd-925-2010-12  wind-vwnd-925-2010-13  \\\n",
       "index                                                                        \n",
       "0                     -27.68                 -37.21                   8.32   \n",
       "1                     -21.13                 -36.57                   8.77   \n",
       "2                     -10.72                 -34.16                   6.99   \n",
       "3                       0.33                 -31.04                   6.17   \n",
       "4                       9.83                 -31.80                   7.47   \n",
       "\n",
       "       wind-vwnd-925-2010-14  wind-vwnd-925-2010-15  wind-vwnd-925-2010-16  \\\n",
       "index                                                                        \n",
       "0                       9.56                  -2.03                  48.13   \n",
       "1                      21.17                   4.44                  48.60   \n",
       "2                      32.16                   5.01                  48.53   \n",
       "3                      39.66                  -1.41                  50.59   \n",
       "4                      38.62                  -5.21                  54.73   \n",
       "\n",
       "       wind-vwnd-925-2010-17  wind-vwnd-925-2010-18  wind-vwnd-925-2010-19  \\\n",
       "index                                                                        \n",
       "0                      28.09                 -13.50                  11.90   \n",
       "1                      27.41                 -23.77                  15.44   \n",
       "2                      19.21                 -33.16                  15.11   \n",
       "3                       8.29                 -37.22                  18.24   \n",
       "4                      -2.58                 -42.30                  21.91   \n",
       "\n",
       "       wind-vwnd-925-2010-20  \n",
       "index                         \n",
       "0                       4.58  \n",
       "1                       3.42  \n",
       "2                       4.82  \n",
       "3                       9.74  \n",
       "4                      10.95  \n",
       "\n",
       "[5 rows x 245 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0CRSEN/LargeData/train_data.csv\", parse_dates=['startdate'], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375734, 245)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "0         28.744480\n",
       "1         28.370585\n",
       "2         28.133059\n",
       "3         28.256798\n",
       "4         28.372353\n",
       "            ...    \n",
       "375729    17.150954\n",
       "375730    16.962051\n",
       "375731    16.915474\n",
       "375732    16.536761\n",
       "375733    15.910995\n",
       "Name: contest-tmp2m-14d__tmp2m, Length: 375734, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'contest-tmp2m-14d__tmp2m'\n",
    "df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the original dataset contains 375,734 observations, 244 features and one target variable `contest-tmp2m-14d__tmp2m`. According to the competition, `contest-tmp2m-14d__tmp2m` is the **arithmetic mean of the max and min observed temperature over the next 14 days for each location and start date**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset\n",
    "\n",
    "This section explains how we preprocessed the original dataset and then split it into train, validation, and test datasets. Preprocessing is a critical step in building a successful Machine Learning project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Location Groups\n",
    "\n",
    "First, the location data given are expressed in terms of longitude and latitude, we can turn this critical information into a more convenient format by creating location groups. A location group has its unique combination of longitude and latitude. However, to avoid having too many different groups, we choose to round the longitudes and latitudes down to contain 14 decimal places. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loc_group(df):\n",
    "    \n",
    "    scale = 14\n",
    "\n",
    "    df['lat'] = np.round(df['lat'], scale)\n",
    "    df['lon'] = np.round(df['lon'], scale)\n",
    "    df['loc_group'] = df.groupby(['lon', 'lat']).ngroup()\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = loc_group(df)\n",
    "df.loc_group.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created in total 514 different location groups for which we will predict the average weather in the next 14 days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731, 731]\n",
      "\n",
      "731\n"
     ]
    }
   ],
   "source": [
    "n_startdates = []\n",
    "\n",
    "for n in range(df.loc_group.nunique()):\n",
    "    n_startdates.append(df[df.loc_group == n].startdate.nunique())\n",
    "\n",
    "print(n_startdates)\n",
    "print()\n",
    "print(df.startdate.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each `loc_group` has 731 unique start dates and we have in total 731 unique start dates in the entire dataset, meaning all location groups have data for the same 731 start dates. This is a good thing because we have a balanced dataset for training models to forecast weather in different locations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine columns with missing values in the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() >0:\n",
    "        na_cols.append(col)\n",
    "        print(col, df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the competition, **cancm30, cancm40, ccsm30, ccsm40, cfsv20, gfdlflora0, gfdlflorb0, gfdl0, nasa0, and nmme0mean** are most recent forecasts from weather models. \n",
    "\n",
    "- `nmme0-prate-34w`: weeks 3-4 weighted average of most recent monthly NMME model forecasts for precipitation.\n",
    "- `nmme0-prate-56w`: weeks 5-6 weighted average of most recent monthly NMME model forecasts for precipitation.\n",
    "- `nmme-prate-34w`: weeks 3-4 weighted average of monthly NMME model forecasts for precipitation.\n",
    "- `nmme-prate-56w`: weeks 5-6 weighted average of monthly NMME model forecasts for precipitation.\n",
    "- `nmme-tmp2m-34w`: weeks 3-4 weighted average of most recent monthly NMME model forecasts for target label, `contest-tmp2m-14d__tmp2m`.\n",
    "- `nmme-tmp2m-56w`: weeks 5-6 weighted average of monthly NMME model forecasts for target label, `contest-tmp2m-14d__tmp2m`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that we are missing forecasts from **ccsm30 and ccsm3** NMME (North American Multi-Model Ensemble) weather models for the above prefixes. \n",
    "\n",
    "We can make an educated guess that the missing forecasts can be derived from the remaining complete forecasts. Specifically, for column `nmme-tmp2m-34w__ccsm3`, its missing values can be computed through first multiplying the average forecast `nmme-tmp2m-34w__nmmemean` by 9 to get the sum of **nmme-tmp2m-34w** forecasts and then subtract the other 8 forecasts (of 8 remaining weather forecast models).\n",
    "\n",
    "We will use this methodology to fill missing values for all columns containing missing values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_na_cols = ['nmme0-tmp2m-34w__ccsm30',\n",
    " 'nmme-tmp2m-56w__ccsm3',\n",
    " 'nmme-prate-34w__ccsm3',\n",
    " 'nmme0-prate-56w__ccsm30',\n",
    " 'nmme0-prate-34w__ccsm30',\n",
    " 'nmme-prate-56w__ccsm3',\n",
    " 'nmme-tmp2m-34w__ccsm3']\n",
    "\n",
    "\n",
    "forecast_mean_cols = ['nmme0-tmp2m-34w__nmme0mean', \n",
    " 'nmme-tmp2m-56w__nmmemean', \n",
    " 'nmme-prate-34w__nmmemean', \n",
    " 'nmme0-prate-56w__nmme0mean', \n",
    " 'nmme0-prate-34w__nmme0mean', \n",
    " 'nmme-prate-56w__nmmemean', \n",
    " 'nmme-tmp2m-34w__nmmemean']\n",
    "\n",
    "\n",
    "nmme0_tmp2m_34w = ['nmme0-tmp2m-34w__cancm30',\n",
    "'nmme0-tmp2m-34w__cancm40',\n",
    "'nmme0-tmp2m-34w__ccsm40',\n",
    "'nmme0-tmp2m-34w__cfsv20',\n",
    "'nmme0-tmp2m-34w__gfdlflora0',\n",
    "'nmme0-tmp2m-34w__gfdlflorb0',\n",
    "'nmme0-tmp2m-34w__gfdl0',\n",
    "'nmme0-tmp2m-34w__nasa0']\n",
    "\n",
    "nmme_tmp2m_56w = ['nmme-tmp2m-56w__cancm3',\n",
    "'nmme-tmp2m-56w__cancm4',\n",
    "'nmme-tmp2m-56w__ccsm4',\n",
    "'nmme-tmp2m-56w__cfsv2',\n",
    "'nmme-tmp2m-56w__gfdl',\n",
    "'nmme-tmp2m-56w__gfdlflora',\n",
    "'nmme-tmp2m-56w__gfdlflorb',\n",
    "'nmme-tmp2m-56w__nasa']\n",
    "\n",
    "nmme_prate_34w = ['nmme-prate-34w__cancm3',\n",
    "'nmme-prate-34w__cancm4',\n",
    "'nmme-prate-34w__ccsm4',\n",
    "'nmme-prate-34w__cfsv2',\n",
    "'nmme-prate-34w__gfdl',\n",
    "'nmme-prate-34w__gfdlflora',\n",
    "'nmme-prate-34w__gfdlflorb',\n",
    "'nmme-prate-34w__nasa']\n",
    "\n",
    "nmme0_prate_56w = [ 'nmme0-prate-56w__cancm30',\n",
    "'nmme0-prate-56w__cancm40',\n",
    "'nmme0-prate-56w__ccsm40',\n",
    "'nmme0-prate-56w__cfsv20',\n",
    "'nmme0-prate-56w__gfdlflora0',\n",
    "'nmme0-prate-56w__gfdlflorb0',\n",
    "'nmme0-prate-56w__gfdl0',\n",
    "'nmme0-prate-56w__nasa0']\n",
    "\n",
    "nmme0_prate_34w = ['nmme0-prate-34w__cancm30',\n",
    "'nmme0-prate-34w__cancm40',\n",
    "'nmme0-prate-34w__ccsm40',\n",
    "'nmme0-prate-34w__cfsv20',\n",
    "'nmme0-prate-34w__gfdlflora0',\n",
    "'nmme0-prate-34w__gfdlflorb0',\n",
    "'nmme0-prate-34w__gfdl0',\n",
    "'nmme0-prate-34w__nasa0']\n",
    "\n",
    "nmme_prate_56w = ['nmme-prate-56w__cancm3',\n",
    "'nmme-prate-56w__cancm4',\n",
    "'nmme-prate-56w__ccsm4',\n",
    "'nmme-prate-56w__cfsv2',\n",
    "'nmme-prate-56w__gfdl',\n",
    "'nmme-prate-56w__gfdlflora',\n",
    "'nmme-prate-56w__gfdlflorb',\n",
    "'nmme-prate-56w__nasa']\n",
    "\n",
    "nmme_tmp2m_34w = ['nmme-tmp2m-34w__cancm3',\n",
    "'nmme-tmp2m-34w__cancm4',\n",
    "'nmme-tmp2m-34w__ccsm4',\n",
    "'nmme-tmp2m-34w__cfsv2',\n",
    "'nmme-tmp2m-34w__gfdl',\n",
    "'nmme-tmp2m-34w__gfdlflora',\n",
    "'nmme-tmp2m-34w__gfdlflorb',\n",
    "'nmme-tmp2m-34w__nasa']\n",
    "\n",
    "forecast_cols = [nmme0_tmp2m_34w, nmme_tmp2m_56w, nmme_prate_34w, nmme0_prate_56w, nmme0_prate_34w, nmme_prate_56w, nmme_tmp2m_34w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df):\n",
    "\n",
    "    for i, na_col in enumerate(forecast_na_cols):\n",
    "        total = df[forecast_mean_cols[i]] * 9\n",
    "        sum_of_8_forecast = df[forecast_cols[i]].sum(axis=1)\n",
    "        df[na_col] = total - sum_of_8_forecast\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = impute_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just drop the `ccsm30` column as it's redundant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_col(df):\n",
    "    \n",
    "    df.drop(['ccsm30'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_redundant_col(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we no longer have missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Lasso Regressions (to Obtain Residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting algorithms are normally used to predict the residuals of the base models' predictions. To obtain these residuals, we can first fit linear regressions to the data with some regularization. To account for the fact that different locations might have different relationships between its predictors and its response variable, we fit 514 different linear regressions and collect their residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, Test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the data used for fitting linear regressions by location group and start date. We split the data so that for each location group, data between 2014-09-1 and 2015-11-15 are used for training, data between 2015-11-15 and 2016-04-08 are used for validation, and data between 2016-04-08 and 2016-08-31 are used for testing. By doing so, we maintain a **6:2:2** ratio of train, validation, and test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['loc_group', 'startdate'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df[\"startdate\"] <= pd.datetime(2015, 11, 15)]\n",
    "val = df[(df[\"startdate\"] > pd.datetime(2015, 11, 15)) & (df[\"startdate\"] <= pd.datetime(2016, 4, 8))]\n",
    "test = df[df[\"startdate\"] > pd.datetime(2016, 4, 8)]\n",
    "\n",
    "print(f'Train_shape: {train.shape}  |  Val_shape: {val.shape}   |  Test_shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection For Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are fitting linear regression models, predictors that have high linear correlation with the response variable will give us better model fitting results. Thus, we select some features based on this criterion, hoping that the selected features could explain any underlying linear relationships.\n",
    "\n",
    "The first set of potential features we will look at are the ones whose names contain \"14d\", which indicates that they are measured over the same time frame as the target variable and thus they have higher chance of being linearly correlated with the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if '14d' in col and col != target:\n",
    "        corr = df[[col, target]].corr().iloc[0,1]\n",
    "        if np.abs(corr) > 0.8: \n",
    "            print(col, corr)\n",
    "            LR_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting 0.8 as the threshold, we identified three features with high linear correlations with the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another potential set of features are the NMME model forecasts as they are purely physics-based models which forecast weather-related information such as precipitation. Hence, they have the potential of providing us with some meaningful insights. We have the following prefixes, each of them is associated with the NMME model forecasts over a specific time frame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixs = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if '34w' in col or '56w' in col:\n",
    "        prefix = col.split(\"__\")[0]\n",
    "        prefixs.append(prefix)\n",
    "\n",
    "prefixs = list(set(prefixs))\n",
    "prefixs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are many columns sharing the same prefix, let's see if information contained in columns sharing a prefix can be \"summarized\" by linear PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "y = np.array(df[target]).reshape(1, -1)\n",
    "\n",
    "for prefix in prefixs:\n",
    "    \n",
    "    cols = [col for col in df.columns if prefix in col and 'mean' not in col]\n",
    "    pca1 = pca.fit_transform(df[cols]).reshape(1, -1)\n",
    "    print(prefix, f\"| pca1 explained variance: {pca.explained_variance_ratio_[0]}\")\n",
    "    print(f\"pca1 vs target corr: {np.corrcoef(pca1, y)[1,0]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we can conclude that for \"nmme0-tmp2m-34\", \"nmme-tmp2m-56w\", and \"nmme-tmp2m-34w\" forecasts, the contained information can be summarized by the first principal component and the first principal component is highly correlated with the target (|correlation| > 0.8). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_prefix = ['nmme0-tmp2m-34w', 'nmme-tmp2m-34w', 'nmme-tmp2m-56w']\n",
    "\n",
    "corr_prefix_cols = [nmme0_tmp2m_34w, nmme_tmp2m_34w, nmme_tmp2m_56w]\n",
    "    \n",
    "for prefix, cols in zip(corr_prefix, corr_prefix_cols):\n",
    "    pc_name = prefix + '-pc'\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(df[cols])\n",
    "    for ds in [train, val, test]:\n",
    "        ds[pc_name] = pca.fit_transform(ds[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cols = LR_cols + [prefix+'-pc' for prefix in corr_prefix]\n",
    "LR_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 6 continuous variables for fitting linear regressions, we can examine the remaining variables and select them as features if they have high linear correlations with our target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    \n",
    "    if df[col].dtype == 'float64' and col not in LR_cols and 'nmme0-tmp2m-34w' not in col and 'nmme-tmp2m-34w' not in col and 'nmme-tmp2m-56w' not in col:      \n",
    "        corr = df[[col, target]].corr().iloc[0,1]\n",
    "        if np.abs(corr) > 0.8:\n",
    "            LR_cols.append(col)\n",
    "            print(col, corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 10 continuous predictors for fitting linear regressions. They are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cols = [col for col in LR_cols if col != target]\n",
    "LR_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Alpha and Fitting Linear Regressions (to Obtain Residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following defined function `LR_models` to help us fit 514 linear regressions to the weather data of 514 different locations. Notice that the function takes three parameters:\n",
    "\n",
    "- `alpha`: float, regularization parameter shared by the 514 linear regressions\n",
    "\n",
    "- `selected_feats`: list of strings, selected features for fitting linear regressions to the data\n",
    "\n",
    "- `shift_feats`: boolean, True if we want to shift the `..-34w-..` and `..-56w-..` features backward by 14 and 28 days respectively, by location group.\n",
    "\n",
    "The function will return the training rmse and validation rmse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_models(alpha, selected_feats, shift_feats):\n",
    "    \n",
    "    # new train and vaidation dataset containing only the selected features\n",
    "    X_train, y_train = train[selected_feats], train[target]\n",
    "    X_val, y_val = val[selected_feats], val[target]\n",
    "\n",
    "    train_se = 0\n",
    "    val_se = 0\n",
    "\n",
    "    # loop through 514 different locations, \n",
    "    # for each location we create a machine learning pipeline and fit a linear regression model\n",
    "    for i in range(df.loc_group.nunique()):\n",
    "\n",
    "        # train and validation dataset for the ith location\n",
    "        X1_train = X_train[train[\"loc_group\"]==i]\n",
    "        y1_train = y_train[train[\"loc_group\"]==i]\n",
    "        X1_val = X_val[val[\"loc_group\"]==i]\n",
    "        y1_val = y_val[val[\"loc_group\"]==i]\n",
    "\n",
    "        # collect the indices of the observations, which we will use to recover the train and val split of the data\n",
    "        t_index = X_train[train[\"loc_group\"]==i].index\n",
    "        v_index = X_val[val[\"loc_group\"]==i].index\n",
    "\n",
    "        # if shift_feats, we combine the train and val set to shift the \"34w\" and \"56w\" features \n",
    "        if shift_feats == True and np.any(['34w' in col or '56w' in col for col in selected_feats]):\n",
    "            X1_conc = pd.concat([X1_train, X1_val])           \n",
    "            for col in selected_feats:\n",
    "                if '34w' in col:\n",
    "                    X1_conc[col] = X1_conc[col].shift(-14).ffill()\n",
    "                if '56w' in col:\n",
    "                    X1_conc[col] = X1_conc[col].shift(-28).ffill()\n",
    "\n",
    "            # we recover the train and val set by their respective list of indices\n",
    "            X1_train = X1_conc.loc[t_index, :]\n",
    "            X1_val = X1_conc.loc[v_index, :]\n",
    "        \n",
    "        # scale the data\n",
    "        ss = StandardScaler()\n",
    "        X1_train_ss = ss.fit_transform(X1_train)\n",
    "        X1_val_ss = ss.transform(X1_val)\n",
    "\n",
    "        # fit the linear regression model with alpha as the regularization parameter\n",
    "        LR = Lasso(alpha=alpha, max_iter=10000)\n",
    "        LR.fit(X1_train_ss, y1_train)\n",
    "        y1_train_pred = LR.predict(X1_train_ss)\n",
    "        y1_val_pred = LR.predict(X1_val_ss)\n",
    "        \n",
    "        # accumulate the total squared errors by the sum of squared errors from the ith location\n",
    "        train_se += mean_squared_error(y1_train, y1_train_pred) * len(t_index)\n",
    "        val_se += mean_squared_error(y1_val, y1_val_pred) * len(v_index)\n",
    "        \n",
    "    # return the root mean squared error of train and validation set\n",
    "    train_rmse = np.round(np.sqrt(train_se / X_train.shape[0]), 3)\n",
    "    val_rmse = np.round(np.sqrt(val_se / X_val.shape[0]), 3)\n",
    "    \n",
    "    return train_rmse, val_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important parameter in fitting Linear Regressions is the `alpha` parameter. Here we are going to try a list of different alphas and then look at the root mean squared error of the training and validation dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmses = []\n",
    "val_rmses = []\n",
    "alphas = [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 0.8, 1, 1.5, 2, 2.5, 3, 3.5, 4]\n",
    "\n",
    "for alpha in alphas:\n",
    "    \n",
    "    print(f\"alpha = {alpha}...\")\n",
    "    train_rmse, val_rmse = LR_models(alpha, LR_cols, True)\n",
    "    train_rmses.append(train_rmse)\n",
    "    val_rmses.append(val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, train_rmses, 'o-', label=\"train rmse\")\n",
    "plt.plot(alphas, val_rmses,'o-', label=\"val rmse\")\n",
    "plt.plot(alphas, np.array(val_rmses)-np.array(train_rmses), 'o-', label=\"val-train rmse\")\n",
    "\n",
    "for i in np.arange(8, len(alphas)):\n",
    "    plt.annotate(f\"{np.round(val_rmses[i]-train_rmses[i], 2)}\", (alphas[i], val_rmses[i]-train_rmses[i]))\n",
    "\n",
    "plt.xlabel(\"lasso regression alpha\")\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the error value to be as small as possible but we also don't want the models to overfit to the training data. Thus, to take both the error metric and the overfitting problem into account, we decide to use **alpha = 2** to create the linear regression residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the residuals from the linear regressions, we are going to modify the `LR_models` function. Note that we don't need the `alpha` argument in the function anymore as it's now a fixed value.\n",
    "\n",
    "To have more training data to learn the 514 linear regressions, we are going to combine `train` and `val` into one new training dataset and we call it `train2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.concat([train, val], axis=0)\n",
    "print(f'Train2_shape: {train2.shape}  |  Test_shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the modified function for obtaining linear regression residuals. It will return the fitted residuals as well as the predictions for the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_models_residuals(selected_feats, shift_feats):\n",
    "    \n",
    "    # new train and test dataset containing only the selected features\n",
    "    X_train, y_train = train2[selected_feats], train2[target]\n",
    "    X_test, y_test = test[selected_feats], test[target]\n",
    "\n",
    "    pred_residuals = np.zeros(df.shape[0])\n",
    "    predictions = np.zeros(df.shape[0])\n",
    "\n",
    "    # loop through 514 different locations, \n",
    "    # for each location we create a machine learning pipeline and fit a linear regression model\n",
    "    for i in range(df.loc_group.nunique()):\n",
    "\n",
    "        # train and test dataset for the ith location\n",
    "        X1_train = X_train[train2[\"loc_group\"]==i]\n",
    "        y1_train = y_train[train2[\"loc_group\"]==i]\n",
    "        X1_test = X_test[test[\"loc_group\"]==i]\n",
    "        y1_test = y_test[test[\"loc_group\"]==i]\n",
    "\n",
    "        # collect the indices of the observations, which we will use to recover the train and test split of the data\n",
    "        t_index = X_train[train2[\"loc_group\"]==i].index\n",
    "        te_index = X_test[test[\"loc_group\"]==i].index\n",
    "\n",
    "        # if shift_feats, we combine the train and test set to shift the \"34w\" and \"56w\" features \n",
    "        if shift_feats == True and np.any(['34w' in col or '56w' in col for col in selected_feats]):\n",
    "            X1_conc = pd.concat([X1_train, X1_test])           \n",
    "            for col in selected_feats:\n",
    "                if '34w' in col:\n",
    "                    X1_conc[col] = X1_conc[col].shift(-14).ffill()\n",
    "                if '56w' in col:\n",
    "                    X1_conc[col] = X1_conc[col].shift(-28).ffill()\n",
    "\n",
    "            # we recover the train and test set by their respective list of indices\n",
    "            X1_train = X1_conc.loc[t_index, :]\n",
    "            X1_test = X1_conc.loc[te_index, :]\n",
    "        \n",
    "        # scale the data\n",
    "        ss = StandardScaler()\n",
    "        X1_train_ss = ss.fit_transform(X1_train)\n",
    "        X1_test_ss = ss.transform(X1_test)\n",
    "\n",
    "        # fit the linear regression model with alpha = 2.0 as the regularization parameter\n",
    "        LR = Lasso(alpha=2.0, max_iter=10000)\n",
    "        LR.fit(X1_train_ss, y1_train)\n",
    "        y1_train_pred = LR.predict(X1_train_ss)\n",
    "        y1_test_pred = LR.predict(X1_test_ss)\n",
    "        predictions[t_index] = y1_train_pred\n",
    "        predictions[te_index] = y1_test_pred\n",
    "        \n",
    "        # return predicted residuals\n",
    "        pred_residuals[t_index] = y1_train - y1_train_pred\n",
    "        pred_residuals[te_index] = y1_test - y1_test_pred\n",
    "    \n",
    "    return pred_residuals, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_residuals, predictions = LR_models_residuals(LR_cols, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirm that we have obtained linear regression fitted residuals for all observations in the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred_residuals) == df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a histogram of the fitted residuals:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_residuals, density=True)\n",
    "plt.xlabel(\"Fitted residuals\")\n",
    "plt.ylabel(\"Normalized count\")\n",
    "plt.title(\"Lassion regression fitted residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Boosting Algorithms to Predict Residuals\n",
    "\n",
    "CatBoost, LightGBM, and XGBoost are three popular boosting algorithms in machine learning. Before we dive into the these three algorithms, let's start with the term ensemble learning and how boosting as one type of ensemble learning helps us reduce model errors.\n",
    "\n",
    "**Ensemble Learning** is a Machine Learning technique that combines predictions from multiple base models to produce more accurate and robust predictions. Ensemble learning can improve the generalization of the model. It can help to create a model that can generalize well to new and unseen data by combining the different perspectives of individual models. There are several ways to create an ensemble model, including **bagging, boosting, and stacking**. \n",
    "\n",
    "**Bagging** is primarily used to reduce the variance of a model by combining predictions of multiple models that are trained on different subsets of the training data through a voting mechanism. **Boosting**, on the other hand, is used to reduce the bias of a model by sequentially training models that focus on correcting the errors of the previous models. **Stacking** is used to combine multiple models by training a meta-model on the predictions of the base models to create more accurate final predictions.\n",
    "\n",
    "In this guided project, **we will focus on the Boosting algorithms**, which are particularly helpful for achieving success in data science competitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we concatenate the fitted residuals to the dataframe and make sure that there are no null values in the `residuals` column as it will be our target column when fitting the Boosting models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['residuals'] = pred_residuals\n",
    "df.residuals.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the `residuals` column to the training and test dataset as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2['residuals'] = df.loc[train2.index, 'residuals']\n",
    "test['residuals'] = df.loc[test.index, 'residuals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection for Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to select features for the Boosting models, which we will call `B_cols`. First, we will use the forecasts from the purely physics-based weather models and we will select the forecast means this time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_means = [col for col in df.columns if 'mean' in col]\n",
    "forecast_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before with linear regressions, we are going to shift the `..-34w-..` and `..-56w-..` features backward by 14 and 28 days respectively, by location group, and then we add those shifted features to `B_cols`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_cols = []\n",
    "\n",
    "for col in forecast_means:\n",
    "    if '34w' in col:\n",
    "        df[col+'_shifted'] = df.groupby(['loc_group'])[col].shift(-14).ffill()\n",
    "        B_cols.append(col+'_shifted')\n",
    "    elif '56w' in col:\n",
    "        df[col+'_shifted'] = df.groupby(['loc_group'])[col].shift(-28).ffill()\n",
    "        B_cols.append(col+'_shifted')   \n",
    "    else:   \n",
    "        B_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we append these shifted features to our training and test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [train2, test]:\n",
    "    for col in B_cols:\n",
    "        ds[col] = df.loc[ds.index, col]\n",
    "\n",
    "print(train2.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another set of continuous variables we could add to `B_cols` are the ones whose names contain \"14d\", because they are measured over the same time interval as our original target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if '14d' in col:\n",
    "        B_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the continuous variables, don't forget about the categorical variables that we haven't touched on so far. The original dataset comes with one categorical variable called `climateregions__climateregion`. We can encode this variable using Label Encoder from scikit-learn. **Note that we can analyze the feature importance later after we fit the models**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = LabelEncoder()\n",
    "LE.fit(train2['climateregions__climateregion'])\n",
    "train2['climateregions__climateregion'] = LE.transform(train2['climateregions__climateregion'])\n",
    "test['climateregions__climateregion'] = LE.transform(test['climateregions__climateregion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a `month` variable to see if some months have larger fitted residuals than the others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['startdate'].dt.month\n",
    "df.groupby(['month'])['residuals'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our linear regression fitted residuals differ quite perceivably for different months, so we could consider `month` as a feature for the Boosting models. Let's create `month` variable for the training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [train2, test]:\n",
    "    ds['month'] = ds['startdate'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loc_group` is also an important categorical variable as it specifies the location of the weather data. According to the competition, `elevation__elevation` is also an important feature, so we will add them to `B_cols`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_cols += ['climateregions__climateregion', 'month', 'loc_group', 'elevation__elevation']\n",
    "\n",
    "print(f\"We have {len(B_cols)} features for fitting the Boosting models.\")\n",
    "B_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Models for Predicting Fitted Residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are no longer looking to explain any linear relationships between the predictors and the target variable (as they have been handled by the linear regressions we fitted), we will move on to using some Boosting models includeing CatBoost, LightGBM, and XGBoost to capture complex non-linear relationships between the input features and the fitted residuals. \n",
    "\n",
    "We will use a combination of three Boosting models to predict our fitted residuals, so basically we are trying to see if there is any non-linear pattern in the fitted residuals that we could explain using Boosting models. If the patterm we found is generalizable to unseen data, we can further reduce the gap between what Machine Learning models can predict and the ground truths. \n",
    "\n",
    "- **CatBoost**: CatBoost works by iteratively building a set of decision trees. At each iteration, it calculates the gradient of the loss function with respect to the current set of predictions, and fits a decision tree to the negative gradient. The predictions from the new decision tree are then added to the current predictions, and the process is repeated for a specified number of iterations. CatBoost also incorporates a unique mechanism for handling categorical variables, which it achieves by using an ordered boosting technique that sorts the categories in a way that optimizes the loss function.\n",
    "\n",
    "- **LightGBM**: LightGBM, on the other hand, uses a different approach for building decision trees. Instead of using a depth-first approach like many other Boosting models, LightGBM uses a leaf-wise approach that grows the tree by expanding the leaf with the largest loss reduction. This approach can be more efficient than depth-first approaches because it reduces the number of data points that need to be processed for each split. LightGBM also uses the Gradient-based One-Side Sampling (GOSS) technique to sample the most important instances for each iteration, which further improves efficiency.\n",
    "\n",
    "- **XGBoost**: XGBoost also builds decision trees, but it uses a slightly different algorithm. Like CatBoost and LightGBM, XGBoost uses a gradient-based approach to build decision trees. However, it also includes several advanced features such as regularization techniques, early stopping, and a second-order approximation of the gradient to improve prediction accuracy. XGBoost also allows for parallel processing, which can be useful for large-scale datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0CRSEN/images/1_V-vikG-ye2Y03R943d0CPw.jpeg\" width=\"50%\"></center>\n",
    "\n",
    "<p style=\"color:gray; text-align:center;\">Credit: enjoyalgorithms.com</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify catrgorical columns for the three Boosting models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols =  ['climateregions__climateregion', 'month', 'loc_group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `boosting` function to help us fit three Boosting models at one. The function takes in three parameter:\n",
    "\n",
    "- `ratios`: list of floats, specifying the weighting of each model in the final predictions.\n",
    "\n",
    "- `selected_cols`: list of strings, selected features for fitting the three Boosting models.\n",
    "\n",
    "- `cat_cols`: list of strings, names of categorical columns in `selected_cols`.\n",
    "\n",
    "The function will return the predicted residuals for the training and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting(ratios=[0.6, 0.3, 0.1], selected_cols=B_cols, cat_cols=cat_cols):\n",
    "    \n",
    "    \n",
    "    X_train, y_train = train2[selected_cols], train2['residuals']\n",
    "    X_test, y_test = test[selected_cols], test['residuals']\n",
    "\n",
    "    for ds in [X_train, X_test]:\n",
    "        ds['loc_group'] = ds['loc_group'].astype(\"category\")\n",
    "        ds['climateregions__climateregion'] = ds['climateregions__climateregion'].astype(\"category\")\n",
    "        ds['month'] = ds['month'].astype(\"category\")\n",
    "\n",
    "\n",
    "    cat_inds = [selected_cols.index(col) for col in cat_cols]\n",
    "\n",
    "    train_pred_cb, train_pred_lgb, train_pred_xgb = [np.zeros(len(y_train))] * 3\n",
    "    test_pred_cb, test_pred_lgb, test_pred_xgb = [np.zeros(len(y_test))] * 3\n",
    "\n",
    "    if ratios[0] > 0:\n",
    "        cbr = cb.CatBoostRegressor(loss_function=\"RMSE\", random_state=42, verbose=1)  \n",
    "        cbr.fit(X_train, y_train, cat_features=cat_inds)\n",
    "        train_pred_cb = cbr.predict(X_train)\n",
    "        test_pred_cb = cbr.predict(X_test)\n",
    "    if ratios[1] > 0:\n",
    "        lgbr = lgb.LGBMRegressor(objective='regression', metric='rmse', random_state=42, verbose=1)       \n",
    "        lgbr.fit(X_train, y_train, categorical_feature=cat_inds)\n",
    "        train_pred_lgb = lgbr.predict(X_train)\n",
    "        test_pred_lgb = lgbr.predict(X_test)\n",
    "    if ratios[2] > 0:\n",
    "        xgbr = xgb.XGBRegressor(objective=\"reg:squarederror\", eval_metric='rmse', enable_categorical=True, random_state=42, verbose=0)\n",
    "        xgbr.fit(X_train, y_train)\n",
    "        train_pred_xgb = xgbr.predict(X_train)\n",
    "        test_pred_xgb = xgbr.predict(X_test)\n",
    "\n",
    "    train_pred = ratios[0] * train_pred_cb + ratios[1] * train_pred_lgb + ratios[2] * train_pred_xgb\n",
    "    test_pred = ratios[0] * test_pred_cb + ratios[1] * test_pred_lgb + ratios[2] * test_pred_xgb\n",
    "\n",
    "    print(f\"train rmse = {rmse(y_train, train_pred)}, test rmse = {rmse(y_test, test_pred)}\")\n",
    "    \n",
    "    return train_pred, test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted residuals are the part of the fitted residuals that could be captured by the complex non-linear Boosting models. The difference between the predicted residuals and our linear regression fitted residuals will be the final error of our predictions.\n",
    "\n",
    "In the next cell, you can alter the `ratios` argument to increase/decrease each model's contribution to the final prediction and check the resulting training and test root mean squared error. \n",
    "\n",
    "As there is a memory limit with this lab and our dataset is quite large, LightGBM and XGBoost training are commented out. According to experience, CatBoost tends to give the best performance. If you have local or cloud training resources, feel free to download this notebook and play with the `ratios` argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The following CatBoost training is expected to take approximately 5 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pure CatBoost:\")\n",
    "train_pred1, test_pred1 = boosting(ratios=[1, 0, 0])\n",
    "# print(\"Pure LightGBM:\")\n",
    "# train_pred2, test_pred2 = boosting(ratios=[0, 1, 0])\n",
    "# print(\"Pure XGBoost:\")\n",
    "# train_pred3, test_pred3 = boosting(ratios=[0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have now obtained the residuals predicted by the Boosting models(s). We can add them back to the predictions by the Lasso Regressions to obtain the final target predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append linear predictions, which were produced by the 514 Lasso Regressions, to our original data frame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['linearPrediction'] = predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append the residuals predicted by the Boosting model(s) to the original data frame as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_inds = train2.index\n",
    "test_inds = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['boostedResiduals'] = 0\n",
    "df.loc[train2_inds, 'boostedResiduals'] = train_pred1\n",
    "df.loc[test_inds, 'boostedResiduals'] = test_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train2_inds\n",
    "del test_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['boostedResiduals'], density=True)\n",
    "plt.xlabel(\"Boosted residuals\")\n",
    "plt.ylabel(\"Normalized count\")\n",
    "plt.title(\"Boosting fitted residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirm that there are null values in the boosted residuals and the linear predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boostedResiduals.isnull().sum(), df.linearPrediction.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final predictions equal linear predictions plus boosted residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['finalPrediction'] = df['linearPrediction'] + df['boostedResiduals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it would be hard to visualize predictions for all 514 locations, we will pick the locations whose forecasts have the smallest root mean squared error (rmse) and we visualize those locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the rmse of all the groups and store the calculations in `boostedresiduals_dict`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boostedresiduals_grouped = df.groupby(['loc_group'])\n",
    "boostedresiduals_dict = {}\n",
    "\n",
    "for name, group in boostedresiduals_grouped:\n",
    "    \n",
    "    rmse = np.sqrt(np.sum(group['boostedResiduals']**2) / group.shape[0])\n",
    "    boostedresiduals_dict[name] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the location groups by the corresponding rmses in ascending order and we pick four best predicted groups:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_boostedresiduals_dict = dict(sorted(boostedresiduals_dict.items(), key=lambda item: item[1]))\n",
    "best_predicted_groups = list(sorted_boostedresiduals_dict.keys())[:4]\n",
    "best_predicted_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the final predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, group in enumerate(best_predicted_groups):\n",
    "    \n",
    "    plt.subplot(2,2,i+1)\n",
    "    train2_inds = train2.loc[train2.loc_group == group].index\n",
    "    test_inds = test.loc[test.loc_group == group].index\n",
    "    \n",
    "    plt.plot(df.loc[df.loc_group == group, 'startdate'], df.loc[df.loc_group == group, target], label='target')\n",
    "    plt.plot(df.loc[test_inds, 'startdate'], df.loc[test_inds, 'finalPrediction'], label='predictions')\n",
    "    \n",
    "    plt.title(f\"Group{group} predictions\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "plt.suptitle(\"Groups that have the smallest average rmse\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have completed this guided project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guided project demonstrates one way of jump-starting your Data Science competition. Data Science or Machine Learning projects should always follow the following cycle:\n",
    "\n",
    "- **Problem formulation**: defining the problem and understanding the business context.\n",
    "\n",
    "- **Data collection**: gathering and acquiring data from various sources.\n",
    "\n",
    "- **Data preprocessing**: cleaning, transforming, and preparing data for analysis.\n",
    "\n",
    "- **Exploratory data analysis**: understanding the structure, contents, and patterns in the data through visualizations and summary statistics.\n",
    "\n",
    "- **Feature engineering**: creating new features from the existing data.\n",
    "\n",
    "- **Model selection**: choosing an appropriate model or algorithm for the problem.\n",
    "\n",
    "- **Model training**: fitting the model to the data and tuning its hyperparameters.\n",
    "\n",
    "- **Model evaluation**: assessing the performance of the model using appropriate metrics and techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, please keep in mind that at each step of the cycle, there exists a million of different solutions and tools for solving the problem. It's a rapidly evolving field, so there is on one fixed set of winning techniques, but as long as you keep learning and practicing you will eventually develop your own set of rules for your Data Science game! Happy Learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Roxanne Li](https://www.linkedin.com/in/roxanne-li/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0CRSEN2517-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributor(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sina Nazeri](https://www.linkedin.com/in/sina-nazeri?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0CRSEN2517-2023-01-01)  \n",
    "[JC Chen](contributor_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2020-07-17|0.1|Sam|Create Lab Template|\n",
    "|2023-04-03|0.1|Roxanne Li|Create Project First Draft|\n",
    "|2023-04-06|0.1|JC Chen|Reviewed Project First Draft|\n",
    "|2023-04-15|0.2|Roxanne Li|Updated Project|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2022 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
